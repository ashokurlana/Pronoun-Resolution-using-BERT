{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Project_CorefSeq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_JRNwGX86po"
      },
      "source": [
        "# Baseline Implementation of Sequential Coreference Resolution using BERT\n",
        "\n",
        "[Literature reference](https://arxiv.org/pdf/1906.03695.pdf)\n",
        "\n",
        "## Setup necessary depedencies\n",
        "This involves downloading pre-trained BERT models and importing required python dependecies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxrTIhmcb0PJ",
        "outputId": "34356326-7c4a-4693-8465-b85674ad4e6b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIQB8Z2s97vn",
        "outputId": "0ca22f4e-857e-44cd-e21f-92c9575129a5"
      },
      "source": [
        "# For BERT dependecies\n",
        "!pip install transformers\n",
        "\n",
        "# For Span Extraction part\n",
        "!pip install allennlp"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: allennlp in /usr/local/lib/python3.7/dist-packages (2.4.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.8 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.0.8)\n",
            "Requirement already satisfied: torchvision<0.10.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.19.5)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.17.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.10.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.0.12)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.1.95)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.41.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.7.0)\n",
            "Requirement already satisfied: torch<1.9.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.8.1+cu101)\n",
            "Requirement already satisfied: overrides==3.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2)\n",
            "Requirement already satisfied: wandb<0.11.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.10.28)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: boto3<2.0,>=1.14 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.17.60)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: transformers<4.6,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.5.1)\n",
            "Requirement already satisfied: spacy<3.1,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp) (3.10.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.10.0,>=0.8.1->allennlp) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp) (1.0.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (20.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (56.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.9.0,>=1.6.0->allennlp) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp) (3.12.4)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (5.4.8)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.5.4)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.13)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (0.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (2.8.1)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (1.0.1)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (5.0.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (0.4.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.1.14)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2020.12.5)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.60 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp) (1.20.60)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp) (0.4.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp) (0.10.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp) (0.10.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp) (0.0.45)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (2.0.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub>=0.0.8->allennlp) (3.4.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb<0.11.0,>=0.10.0->allennlp) (4.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<4.6,>=4.1->allennlp) (2.4.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.11.0,>=0.10.0->allennlp) (4.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoilFOh_-11m"
      },
      "source": [
        "import numpy as np # For Linear Algebra ops\n",
        "import pandas as pd # Loading GAP dataset\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from __future__ import absolute_import, division, print_function # Stuff \n",
        "\n",
        "import collections # Standard python lib but not used \n",
        "import logging \n",
        "import math \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset # For data handling in PyTorch NN\n",
        "\n",
        "# For training\n",
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "from tqdm import trange\n",
        "\n",
        "from allennlp.modules.span_extractors import SelfAttentiveSpanExtractor # To utilize the full span of the (and their context vectors) of each name candidate. \n",
        "# Mostly target pronouns are always tokenized into single token, so we only need to extract one context vector per pronoun.\n",
        "\n",
        "from transformers import BertTokenizer, BertConfig, BertModel, AdamW  # Adam optimizer \n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr6TcTx6AL5d"
      },
      "source": [
        "## Handling GAP data\n",
        "The GAP dataset is hosted on github and so we need to download it and load it into memory in an appropriate container. Since, this was provided as a part of a shared task, we have access to only the development, validation and test data. So, we need to combine development and validation data sets to create a train data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Mrn0Zaz0CVX1",
        "outputId": "b0400303-d2a4-452c-9729-6a6d530bb19b"
      },
      "source": [
        "# GAP data set urls\n",
        "test_data_url = \"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv\"\n",
        "dev_data_url = \"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\"\n",
        "val_data_url = \"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\"\n",
        "\n",
        "# Load tsv data into memory using Panda's dataframe\n",
        "development_df = pd.read_csv(test_data_url, delimiter=\"\\t\")\n",
        "test_df = pd.read_csv(dev_data_url, delimiter=\"\\t\")\n",
        "validation_df = pd.read_csv(val_data_url, delimiter=\"\\t\")\n",
        "\n",
        "# Print one of the data frames to depict data structure\n",
        "development_df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Text</th>\n",
              "      <th>Pronoun</th>\n",
              "      <th>Pronoun-offset</th>\n",
              "      <th>A</th>\n",
              "      <th>A-offset</th>\n",
              "      <th>A-coref</th>\n",
              "      <th>B</th>\n",
              "      <th>B-offset</th>\n",
              "      <th>B-coref</th>\n",
              "      <th>URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test-1</td>\n",
              "      <td>Upon their acceptance into the Kontinental Hoc...</td>\n",
              "      <td>His</td>\n",
              "      <td>383</td>\n",
              "      <td>Bob Suter</td>\n",
              "      <td>352</td>\n",
              "      <td>False</td>\n",
              "      <td>Dehner</td>\n",
              "      <td>366</td>\n",
              "      <td>True</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Jeremy_Dehner</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test-2</td>\n",
              "      <td>Between the years 1979-1981, River won four lo...</td>\n",
              "      <td>him</td>\n",
              "      <td>430</td>\n",
              "      <td>Alonso</td>\n",
              "      <td>353</td>\n",
              "      <td>True</td>\n",
              "      <td>Alfredo Di St*fano</td>\n",
              "      <td>390</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Norberto_Alonso</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test-3</td>\n",
              "      <td>Though his emigration from the country has aff...</td>\n",
              "      <td>He</td>\n",
              "      <td>312</td>\n",
              "      <td>Ali Aladhadh</td>\n",
              "      <td>256</td>\n",
              "      <td>True</td>\n",
              "      <td>Saddam</td>\n",
              "      <td>295</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Aladhadh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test-4</td>\n",
              "      <td>At the trial, Pisciotta said: ``Those who have...</td>\n",
              "      <td>his</td>\n",
              "      <td>526</td>\n",
              "      <td>Alliata</td>\n",
              "      <td>377</td>\n",
              "      <td>False</td>\n",
              "      <td>Pisciotta</td>\n",
              "      <td>536</td>\n",
              "      <td>True</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Gaspare_Pisciotta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test-5</td>\n",
              "      <td>It is about a pair of United States Navy shore...</td>\n",
              "      <td>his</td>\n",
              "      <td>406</td>\n",
              "      <td>Eddie</td>\n",
              "      <td>421</td>\n",
              "      <td>True</td>\n",
              "      <td>Rock Reilly</td>\n",
              "      <td>559</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Chasers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>test-1996</td>\n",
              "      <td>The sole exception was Wimbledon, where she pl...</td>\n",
              "      <td>She</td>\n",
              "      <td>479</td>\n",
              "      <td>Goolagong Cawley</td>\n",
              "      <td>400</td>\n",
              "      <td>True</td>\n",
              "      <td>Peggy Michel</td>\n",
              "      <td>432</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Evonne_Goolagong_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>test-1997</td>\n",
              "      <td>According to news reports, both Moore and Fily...</td>\n",
              "      <td>her</td>\n",
              "      <td>338</td>\n",
              "      <td>Esther Sheryl Wood</td>\n",
              "      <td>263</td>\n",
              "      <td>True</td>\n",
              "      <td>Barbara Morgan</td>\n",
              "      <td>404</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Hastings_Arthur_Wise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>test-1998</td>\n",
              "      <td>In June 2009, due to the popularity of the Sab...</td>\n",
              "      <td>She</td>\n",
              "      <td>328</td>\n",
              "      <td>Kayla</td>\n",
              "      <td>364</td>\n",
              "      <td>True</td>\n",
              "      <td>Natasha Henstridge</td>\n",
              "      <td>412</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Raya_Meddine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>test-1999</td>\n",
              "      <td>She was delivered to the Norwegian passenger s...</td>\n",
              "      <td>she</td>\n",
              "      <td>305</td>\n",
              "      <td>Irma</td>\n",
              "      <td>255</td>\n",
              "      <td>True</td>\n",
              "      <td>Bergen</td>\n",
              "      <td>274</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/SS_Irma_(1905)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>test-2000</td>\n",
              "      <td>Meg and Vicky each have three siblings, and ha...</td>\n",
              "      <td>her</td>\n",
              "      <td>275</td>\n",
              "      <td>Vicky Austin</td>\n",
              "      <td>217</td>\n",
              "      <td>True</td>\n",
              "      <td>Polly O'Keefe</td>\n",
              "      <td>260</td>\n",
              "      <td>False</td>\n",
              "      <td>http://en.wikipedia.org/wiki/Vicky_Austin</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             ID  ...                                                URL\n",
              "0        test-1  ...         http://en.wikipedia.org/wiki/Jeremy_Dehner\n",
              "1        test-2  ...       http://en.wikipedia.org/wiki/Norberto_Alonso\n",
              "2        test-3  ...              http://en.wikipedia.org/wiki/Aladhadh\n",
              "3        test-4  ...     http://en.wikipedia.org/wiki/Gaspare_Pisciotta\n",
              "4        test-5  ...               http://en.wikipedia.org/wiki/Chasers\n",
              "...         ...  ...                                                ...\n",
              "1995  test-1996  ...  http://en.wikipedia.org/wiki/Evonne_Goolagong_...\n",
              "1996  test-1997  ...  http://en.wikipedia.org/wiki/Hastings_Arthur_Wise\n",
              "1997  test-1998  ...          http://en.wikipedia.org/wiki/Raya_Meddine\n",
              "1998  test-1999  ...        http://en.wikipedia.org/wiki/SS_Irma_(1905)\n",
              "1999  test-2000  ...          http://en.wikipedia.org/wiki/Vicky_Austin\n",
              "\n",
              "[2000 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jImVjWsADZQp"
      },
      "source": [
        "Apart from this, we also define a few PyTorch primitives (classes and functions) to help handle GAP data during modelling phase. \n",
        "\n",
        "One point to note is that since the Dataset is very structured, we don't need to worry about generating all possible spans for coreference resolution. Instead, we focus on constructing spans for the A and B resolution targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJaIENkfDtyS"
      },
      "source": [
        "# Create an integer mapping for Gendered Pronouns in English\n",
        "GENDER_MAPPING = {'he': 0, 'him': 0, 'his': 0,\n",
        "                  'her': 1, 'she': 1, 'hers': 1,\n",
        "                  'it': 2, 'that': 2} # 'it' and 'that' were added for sake of completeness\n",
        "\n",
        "\n",
        "def add_gender_target_details(df):\n",
        "  \"\"\"\n",
        "  Modify given data frame to add pronoun gender\n",
        "  and target data.\n",
        "  \n",
        "  The pronoun gender column maps the target pronoun to an\n",
        "  integer code defined in `GENDER_MAPPING`.\n",
        "\n",
        "  The target column provides a simplied representation of the\n",
        "  A-coref and B-coref columns found in the GAP dataset.\n",
        "  \"\"\"\n",
        "  entries = len(df['ID'])\n",
        "  # Introduce new columns in the data frame\n",
        "  df['Gender'] = pd.Series(np.ones(entries, dtype=int), index=df.index)\n",
        "  df['Target'] = pd.Series(np.ones(entries, dtype=int), index=df.index)\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    df.loc[index, 'Gender'] = GENDER_MAPPING[row['Pronoun'].lower()]\n",
        "    if row['A-coref']:\n",
        "      df.loc[index, 'Target'] = 0\n",
        "    elif row['B-coref']:\n",
        "      df.loc[index, 'Target'] = 1\n",
        "    else:\n",
        "      df.loc[index, 'Target'] = 2\n",
        "  \n",
        "  return None # redundant statment. Needed for compiling code :/\n",
        "\n",
        "\n",
        "def tokenize_data(row, tokenizer):\n",
        "  break_points = sorted([(\"A\", row[\"A-offset\"], row[\"A\"]),\n",
        "                         (\"B\", row[\"B-offset\"], row[\"B\"]),\n",
        "                         (\"P\", row[\"Pronoun-offset\"], row[\"Pronoun\"]),\n",
        "                         ], key=lambda x: x[1])\n",
        "  tokens, spans, current_pos = [], {}, 0\n",
        "  for name, offset, text in break_points:\n",
        "    tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n",
        "        \n",
        "    # Tokenize the target\n",
        "    tmp_tokens = tokenizer.tokenize(row[\"Text\"][offset:offset+len(text)])\n",
        "\n",
        "    spans[name] = [len(tokens) - 2 , len(tokens) + len(tmp_tokens) + 1] # inclusive\n",
        "    tokens.extend(tmp_tokens)\n",
        "    current_pos = offset + len(text)\n",
        "  tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n",
        "\n",
        "  # Handle edge cases for span indices\n",
        "  for key in spans.keys():\n",
        "    start_idx = spans[key][0]\n",
        "    end_idx = spans[key][-1]\n",
        "    if start_idx > end_idx:\n",
        "      start_idx = -1\n",
        "      end_idx = -1\n",
        "    if start_idx < 0:\n",
        "      start_idx = 0\n",
        "    if end_idx >= len(tokens):\n",
        "      end_idx = len(tokens) -1\n",
        "    \n",
        "    spans[key][0] = start_idx\n",
        "    spans[key][1] = end_idx\n",
        "\n",
        "  return tokens, (spans[\"A\"] + spans[\"B\"] + [spans[\"P\"][0]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# PyTorch Dataset specialization\n",
        "class ModelDataset(Dataset):\n",
        "  \"\"\"Custom Dataset for GAP datasets\"\"\"\n",
        "  \n",
        "  def __init__(self, df, tokenizer, is_data_labelled=True):\n",
        "    # Modify current df to add pronoun gender and target information\n",
        "    self.is_labelled = is_data_labelled\n",
        "    add_gender_target_details(df)\n",
        "    \n",
        "    # Extract expected output from dataset if labelled\n",
        "    if is_data_labelled:\n",
        "      self._outputs = df['Target'].values.astype(\"uint8\")\n",
        "\n",
        "    # For each data entry in the data frame, tokenize data and extract out inputs to\n",
        "    # coreference model   \n",
        "    self._offsets, self._tokens, self._ids = [], [], []\n",
        "    for _, row in df.iterrows():\n",
        "      tokens, offsets = tokenize_data(row, tokenizer)\n",
        "      self._offsets.append(offsets)\n",
        "      self._tokens.append(\n",
        "          tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens + [\"[SEP]\"]))\n",
        "      self._ids.append(row['ID'])\n",
        "      \n",
        "  def __len__(self):\n",
        "    return len(self._tokens)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if self.is_labelled:\n",
        "      return self._ids[idx], self._tokens[idx], self._offsets[idx], self._outputs[idx]\n",
        "    return self._ids[idx], self._tokens[idx], self._offsets[idx], None"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gaQdsQVVhGh"
      },
      "source": [
        "## Building the Neural Layer\n",
        "Here, we deal with the details of constructing a Neural Coreference resolution system which uses BERT embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcN9WsG6VgeB"
      },
      "source": [
        "class AntecedentScorer(nn.Module):\n",
        "  \"\"\"\n",
        "  NN layer for handling antecedent ranking\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_size):\n",
        "    super().__init__()\n",
        "    self._inp_size = input_size\n",
        "    self._span_extractor = SelfAttentiveSpanExtractor(input_size)\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(input_size * 3, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 3)\n",
        "    )\n",
        "\n",
        "  def forward(self, inputs, offsets):\n",
        "    span_embds = self._span_extractor(\n",
        "            inputs, \n",
        "            offsets[:, :4].reshape(-1, 2, 2))\n",
        "    target_pn = torch.gather(\n",
        "        inputs, 1, offsets[:, [4]].unsqueeze(2).expand(-1, -1, self._inp_size))\n",
        "    input = torch.cat([span_embds, target_pn], dim=1)\n",
        "    input = input.reshape(offsets.size()[0], -1)\n",
        "    return self.fc(input)\n",
        "\n",
        "\n",
        "class GAPResolver(nn.Module):\n",
        "  \"\"\"\n",
        "  Neural Net for performing pronoun resolution on GAP dataset\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, bert_model, device):\n",
        "    super().__init__()\n",
        "    self._device = torch_device\n",
        "    if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n",
        "      self._bert_hidden_size = 768\n",
        "    elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n",
        "      self._bert_hidden_size = 1024\n",
        "    # Bare Bert Model transformer outputting raw hidden-states without any specific head on top\n",
        "    self._bert = BertModel.from_pretrained(bert_model).to(device)\n",
        "\n",
        "    self._ac_scorer = AntecedentScorer(self._bert_hidden_size).to(device)\n",
        "\n",
        "  def forward(self, tok_tnsr, offsets):\n",
        "    tok_tnsr = tok_tnsr.to(self._device)\n",
        "    bert_outputs = self._bert(tok_tnsr, attention_mask=(tok_tnsr > 0).long(), \n",
        "            token_type_ids=None)\n",
        "    inputs = bert_outputs.last_hidden_state\n",
        "    fin_outs = self._ac_scorer(inputs, offsets.to(self._device))\n",
        "\n",
        "    return fin_outs\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJJed6Xkvwz0"
      },
      "source": [
        "## Putting it all together\n",
        "With all the core components defined, we now need to add some code to train the neural net for coreference resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7jilZJhwJr8"
      },
      "source": [
        "def collate_examples(batch, truncate_len=490):\n",
        "  \"\"\" For handling data in batches\n",
        "  1. Pad the sequences\n",
        "  2. Transform the target.\n",
        "  \"\"\"\n",
        "  \n",
        "  transposed = list(zip(*batch))\n",
        "  id = transposed[0]\n",
        "  max_len = min( max((len(x) for x in transposed[1])), truncate_len)\n",
        "  \n",
        "  tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n",
        "  for i, row in enumerate(transposed[1]):\n",
        "    row = np.array(row[:truncate_len])\n",
        "    tokens[i, :len(row)] = row\n",
        "  \n",
        "  token_tensor = torch.from_numpy(tokens)\n",
        "  # Offsets\n",
        "  offsets = torch.stack([torch.LongTensor(x) for x in transposed[2]], dim=0) + 1 # Account for the [CLS] token\n",
        "  # Labels\n",
        "  if len(transposed) == 3 or transposed[3][0] is None:\n",
        "    return (id, token_tensor, offsets, None)\n",
        "  labels = torch.LongTensor(transposed[3])\n",
        "  return (id, token_tensor, offsets, labels)\n",
        "\n",
        "\n",
        "# Define function for training the NN for coreference resolution\n",
        "def train(dev_df, val_df, tokenizer, model, device, epochs, learn_rate, train_batch_size, val_batch_size):\n",
        "  # Convert Pandas dataframe into PyTorch Dataset\n",
        "  # Since we don't have a decdicated train data set, we will combine the development\n",
        "  # and validation data sets and do stratified 5 folding on them\n",
        "  train_df = pd.concat([dev_df, val_df])\n",
        "  kfold = StratifiedKFold(n_splits=5) \n",
        "\n",
        "  # Creating a copy of the DF for KFold loop\n",
        "  kf_df = test_df.copy()\n",
        "  add_gender_target_details(kf_df) # We split our training data based on Gender of pronoun\n",
        "\n",
        "  # Some logic to set different layers as trainable\n",
        "  def get_children(model):\n",
        "    return model if isinstance(model, (list, tuple)) else list(model.children())\n",
        "  \n",
        "  def set_trainable_attr(model, flag):\n",
        "    model.trainable = flag\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = flag\n",
        "      \n",
        "  def apply_leaf(model, func):\n",
        "    c = get_children(model)\n",
        "    if isinstance(model, nn.Module):\n",
        "      func(model)\n",
        "    if len(c) > 0:\n",
        "      for l in c:\n",
        "        apply_leaf(l, func)\n",
        "  \n",
        "  def set_trainable(l, flag):\n",
        "    apply_leaf(l, lambda model: set_trainable_attr(model, flag))\n",
        "    \n",
        "  set_trainable(model._bert, True)\n",
        "  set_trainable(model._ac_scorer, True)\n",
        "  # For fine-tuning?\n",
        "  #for i in range(12,24):\n",
        "  #  set_trainable(model._bert.encoder.layer[i], True)\n",
        "    \n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "  loss_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  fold_count = 1\n",
        "  val_preds, train_loss, val_loss, val_outs = [], [], [], []\n",
        "  for train_index, valid_index in kfold.split(kf_df, kf_df[\"Gender\"]):\n",
        "    print(\"~~~~~\" * 20)\n",
        "    print(f\"Fold {fold_count}\")\n",
        "    print(\"~~~~\" * 20)\n",
        "\n",
        "    fold_count += 1 # A premature update of fold count\n",
        "\n",
        "    # Creating test and validation data sets\n",
        "    train_ds = ModelDataset(train_df.iloc[train_index], tokenizer)\n",
        "    train_loader = DataLoader(train_ds,\n",
        "                              collate_fn = collate_examples,\n",
        "                              batch_size=train_batch_size,\n",
        "                              num_workers=2,\n",
        "                              pin_memory=True,\n",
        "                              drop_last=False)\n",
        "    val_ds = ModelDataset(train_df.iloc[valid_index], tokenizer)\n",
        "    val_loader = DataLoader(val_ds,\n",
        "                            collate_fn = collate_examples,\n",
        "                            batch_size=val_batch_size,\n",
        "                            num_workers=2,\n",
        "                            pin_memory=True)\n",
        "    \n",
        "    for _ in trange(epochs):\n",
        "      model.train()\n",
        "\n",
        "      # Train in batches\n",
        "      tr_loss, val_acc = 0, 0\n",
        "      train_steps, val_steps = 0, 0\n",
        "      for step, batch in enumerate(train_loader):\n",
        "        # Link batch to device\n",
        "        batch = batch[1:]\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Reset Optimizer Gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get batch data\n",
        "        tokens, offsets, label = batch\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(tokens, offsets)\n",
        "        batch_loss = loss_criterion(predictions, label) \n",
        "        train_loss.append(batch_loss.item())\n",
        "\n",
        "        # Backward propagation\n",
        "        batch_loss.backward()\n",
        "\n",
        "        # Take a step along computed gradient\n",
        "        optimizer.step()\n",
        "\n",
        "        tr_loss += train_loss[-1]\n",
        "        train_steps += 1\n",
        "      \n",
        "      print(\"Average Training Loss: {} for {} batches\".format(tr_loss / train_steps, train_steps))\n",
        "      \n",
        "      # Switch over to batch validation\n",
        "      model.eval()\n",
        "      for batch in val_loader:\n",
        "        # Link batch to device\n",
        "        batch = batch[1:]\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Reset Optimizer Gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get batch data\n",
        "        tokens, offset, label = batch\n",
        "\n",
        "        # We don't want to comupute gradient descent\n",
        "        with torch.no_grad():\n",
        "          predictions = model(tokens, offset)\n",
        "          val_outs.append(label.cpu().numpy())\n",
        "          val_preds.append(torch.nn.functional.softmax(predictions, -1).clamp(1e-4, 1-1e-4).cpu().numpy())\n",
        "          loss = loss_criterion(predictions, label)\n",
        "          val_loss.append(loss.item())\n",
        "\n",
        "          val_acc += val_loss[-1]\n",
        "          val_steps +=1 \n",
        "      \n",
        "      print(\"Average Validation Accuracy: {} for {} batches\".format(val_acc / val_steps, val_steps))\n",
        "\n",
        "    # Add code for train and validation\n",
        "  return val_preds, val_loss, train_loss, model\n",
        "\n",
        "def evaluate_test_data(test_df, model, tokenizer, test_batch_size, device):\n",
        "  test_ds = ModelDataset(test_df, tokenizer)\n",
        "  test_loader = DataLoader(test_ds, \n",
        "                           collate_fn =collate_examples,\n",
        "                           batch_size=test_batch_size,\n",
        "                           num_workers=2,\n",
        "                           pin_memory=True,\n",
        "                           shuffle=False)\n",
        "  \n",
        "  test_preds = []\n",
        "  model.eval()\n",
        "\n",
        "  for batch in test_loader:\n",
        "    # Link batch to device\n",
        "    ids = batch[0]\n",
        "    batch = batch[1:]\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Get batch data\n",
        "    tokens, offset, label = batch\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      predictions = model(tokens, offset)\n",
        "\n",
        "      label = label.cpu().numpy()\n",
        "      preds = torch.nn.functional.softmax(predictions, -1).clamp(1e-4, 1-1e-4).cpu().numpy()\n",
        "      for i in range(len(ids)):\n",
        "        id = ids[i]\n",
        "        target = label[i]\n",
        "        pred = preds[i]\n",
        "        test_preds.append({id: {'target': target, 'prediction': pred}})\n",
        "\n",
        "\n",
        "  return test_preds"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ__ONRSQbSo"
      },
      "source": [
        "## CorefSeq in Action\n",
        "Finally, we connect all pieces together and train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP0KFhWgN2uL",
        "outputId": "9955483e-580b-43ea-fa00-84fbf7e0a100"
      },
      "source": [
        "import time\n",
        "\n",
        "# Create torch tensor for device\n",
        "torch_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "bert_model_type = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_type, do_lower_case=True, never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\"))\n",
        "model = GAPResolver(bert_model_type, torch_device)\n",
        "\n",
        "#model.load_state_dict(torch.load(\"./saved_models/baseline_gap_model\"))\n",
        "\n",
        "# Training Model on given data\n",
        "train_start_time = time.time() * 1000\n",
        "val_preds, val_loss, train_loss, model = train(development_df, validation_df,\n",
        "                                               tokenizer, model, torch_device,\n",
        "                                               15, 1e-5, 10, 32)\n",
        "train_end_time = time.time() * 1000\n",
        "\n",
        "print(\"Time for training: {} ms\".format(train_end_time - train_start_time))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Fold 1\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1763: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss: 0.958288985863328 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 1/15 [00:43<10:04, 43.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.8409944222523615 for 13 batches\n",
            "Average Training Loss: 0.6769433764740824 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 2/15 [01:28<09:27, 43.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.5712034060404851 for 13 batches\n",
            "Average Training Loss: 0.39215081962756815 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 3/15 [02:14<08:54, 44.55s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.5879032485760175 for 13 batches\n",
            "Average Training Loss: 0.1994369146297686 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 4/15 [03:00<08:16, 45.11s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.6798873910537133 for 13 batches\n",
            "Average Training Loss: 0.1057316745922435 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 5/15 [03:47<07:36, 45.65s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.67845152089229 for 13 batches\n",
            "Average Training Loss: 0.056625592816271816 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 6/15 [04:34<06:52, 45.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.7351138488604472 for 13 batches\n",
            "Average Training Loss: 0.026400737966469023 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 7/15 [05:21<06:09, 46.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.8021753464753811 for 13 batches\n",
            "Average Training Loss: 0.015704687527613715 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 53%|█████▎    | 8/15 [06:07<05:23, 46.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.8598805006880027 for 13 batches\n",
            "Average Training Loss: 0.008857141900807618 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 9/15 [06:54<04:38, 46.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.9135048217498339 for 13 batches\n",
            "Average Training Loss: 0.01372988889052067 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 10/15 [07:40<03:52, 46.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.931015646801545 for 13 batches\n",
            "Average Training Loss: 0.01300792220645235 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 11/15 [08:27<03:05, 46.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.9565372237792382 for 13 batches\n",
            "Average Training Loss: 0.013341928262889269 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 12/15 [09:13<02:19, 46.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.9830866490419095 for 13 batches\n",
            "Average Training Loss: 0.015183712556245154 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 13/15 [10:00<01:33, 46.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.9672655761241913 for 13 batches\n",
            "Average Training Loss: 0.010740587284635695 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 14/15 [10:47<00:46, 46.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 1.0056672342694724 for 13 batches\n",
            "Average Training Loss: 0.0031549727881611035 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [11:33<00:00, 46.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 1.0211418271064758 for 13 batches\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Fold 2\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss: 0.2217607130529359 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 1/15 [00:46<10:56, 46.88s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.005886858902298487 for 13 batches\n",
            "Average Training Loss: 0.0655596659409639 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 2/15 [01:33<10:09, 46.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.004862012090877845 for 13 batches\n",
            "Average Training Loss: 0.02086710789517383 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 3/15 [02:20<09:21, 46.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.0033382712236533943 for 13 batches\n",
            "Average Training Loss: 0.012788765174991567 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 4/15 [03:06<08:34, 46.74s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.007393418179932409 for 13 batches\n",
            "Average Training Loss: 0.009614279092238576 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 5/15 [03:53<07:47, 46.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.002567300877462213 for 13 batches\n",
            "Average Training Loss: 0.003952711708006973 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 6/15 [04:40<07:00, 46.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.001938638606449016 for 13 batches\n",
            "Average Training Loss: 0.002012336052757746 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 7/15 [05:27<06:14, 46.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.002345608659267712 for 13 batches\n",
            "Average Training Loss: 0.0052827766738118955 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 53%|█████▎    | 8/15 [06:13<05:27, 46.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.002834900215160675 for 13 batches\n",
            "Average Training Loss: 0.0063020003063684275 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 9/15 [07:00<04:40, 46.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.0037853720602400312 for 13 batches\n",
            "Average Training Loss: 0.0017852271658739482 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 10/15 [07:47<03:53, 46.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.012609692477361443 for 13 batches\n",
            "Average Training Loss: 0.0016766657774951454 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 11/15 [08:34<03:07, 46.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.0013373802027602394 for 13 batches\n",
            "Average Training Loss: 0.0010669680940736726 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 12/15 [09:21<02:20, 46.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.009255979534198279 for 13 batches\n",
            "Average Training Loss: 0.002436991043123271 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 13/15 [10:08<01:33, 46.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.005061068533485433 for 13 batches\n",
            "Average Training Loss: 0.0009736553472976083 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 14/15 [10:54<00:46, 46.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.0037189577974808905 for 13 batches\n",
            "Average Training Loss: 0.0006245094515179517 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [11:41<00:00, 46.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.0013801426108469828 for 13 batches\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Fold 3\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss: 0.013220405990296058 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 1/15 [00:47<10:59, 47.08s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.0008580285771481263 for 13 batches\n",
            "Average Training Loss: 0.007866737923723121 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 2/15 [01:33<10:10, 46.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.000224181040091655 for 13 batches\n",
            "Average Training Loss: 0.01975283504825711 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 3/15 [02:20<09:22, 46.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.0009152453567367047 for 13 batches\n",
            "Average Training Loss: 0.003389641381727415 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 4/15 [03:07<08:36, 46.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.00026092327271516505 for 13 batches\n",
            "Average Training Loss: 0.0011569778281227626 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 5/15 [03:54<07:48, 46.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.00019249435102280515 for 13 batches\n",
            "Average Training Loss: 0.004606316414219691 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 6/15 [04:41<07:02, 46.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.00026320161011356575 for 13 batches\n",
            "Average Training Loss: 0.0011317124680999767 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 7/15 [05:28<06:14, 46.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.00018712747204027927 for 13 batches\n",
            "Average Training Loss: 0.004747668070513101 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 53%|█████▎    | 8/15 [06:14<05:28, 46.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.0008243496424536436 for 13 batches\n",
            "Average Training Loss: 0.0011795750666351522 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 9/15 [07:01<04:41, 46.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.00018657175342713555 for 13 batches\n",
            "Average Training Loss: 0.00040062965662173154 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 10/15 [07:48<03:54, 46.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.00015857002934073814 for 13 batches\n",
            "Average Training Loss: 0.00031404112706923115 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 11/15 [08:35<03:07, 46.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.00012390839737445975 for 13 batches\n",
            "Average Training Loss: 0.007737730181440838 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 12/15 [09:22<02:20, 46.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.015748504427812386 for 13 batches\n",
            "Average Training Loss: 0.012105705311387282 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 13/15 [10:09<01:33, 46.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.0007918249220193292 for 13 batches\n",
            "Average Training Loss: 0.0009360734841720841 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 14/15 [10:56<00:46, 46.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.0003066354309759425 for 13 batches\n",
            "Average Training Loss: 0.0029359435976289206 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [11:42<00:00, 46.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.0037233176433521574 for 13 batches\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Fold 4\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss: 0.0045365424881765645 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 1/15 [00:47<11:03, 47.37s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.00012737563967955514 for 13 batches\n",
            "Average Training Loss: 0.004187617772879548 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 2/15 [01:33<10:12, 47.11s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.0009346154735025126 for 13 batches\n",
            "Average Training Loss: 0.004254275124151263 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 3/15 [02:20<09:24, 47.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.0005739533684950752 for 13 batches\n",
            "Average Training Loss: 0.005665647909518156 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 4/15 [03:07<08:36, 46.95s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.000733032832403506 for 13 batches\n",
            "Average Training Loss: 0.00244438955660371 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 5/15 [03:54<07:48, 46.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.00011197373113156154 for 13 batches\n",
            "Average Training Loss: 0.0024037234681486552 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 6/15 [04:40<07:01, 46.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.00025826802280230017 for 13 batches\n",
            "Average Training Loss: 0.0007978317893730491 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 7/15 [05:27<06:14, 46.82s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 9.60553427900707e-05 for 13 batches\n",
            "Average Training Loss: 0.00025531347201876995 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 53%|█████▎    | 8/15 [06:14<05:27, 46.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 8.668251183945149e-05 for 13 batches\n",
            "Average Training Loss: 0.0006971542915607642 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 9/15 [07:01<04:40, 46.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.0001428762797541612 for 13 batches\n",
            "Average Training Loss: 0.0003636313417018755 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 10/15 [07:48<03:54, 46.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 5.28521153543037e-05 for 13 batches\n",
            "Average Training Loss: 0.0001479737877559728 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 11/15 [08:35<03:07, 46.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 5.080100198448725e-05 for 13 batches\n",
            "Average Training Loss: 0.0002744060963323136 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 12/15 [09:22<02:20, 46.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 4.918883377495849e-05 for 13 batches\n",
            "Average Training Loss: 0.00011774061092069132 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 13/15 [10:08<01:33, 46.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 5.316138329082885e-05 for 13 batches\n",
            "Average Training Loss: 9.521567916408458e-05 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 14/15 [10:55<00:46, 46.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 5.046901473038955e-05 for 13 batches\n",
            "Average Training Loss: 0.00012848915990559817 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [11:42<00:00, 46.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 3.286843345379636e-05 for 13 batches\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Fold 5\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Training Loss: 0.0015806850761777014 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 1/15 [00:47<11:06, 47.60s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 4.97498781791924e-05 for 13 batches\n",
            "Average Training Loss: 0.004998047873669975 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 2/15 [01:34<10:14, 47.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 8.141126363625517e-05 for 13 batches\n",
            "Average Training Loss: 0.010788063883194355 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 3/15 [02:21<09:26, 47.21s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 0.00013087596016703174 for 13 batches\n",
            "Average Training Loss: 0.0007655439669633779 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 4/15 [03:08<08:38, 47.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 4.389218436419749e-05 for 13 batches\n",
            "Average Training Loss: 0.0005232600414274202 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 5/15 [03:55<07:50, 47.09s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 5.2026302280585064e-05 for 13 batches\n",
            "Average Training Loss: 0.00021228285123839897 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 6/15 [04:42<07:03, 47.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 4.331073229807841e-05 for 13 batches\n",
            "Average Training Loss: 0.00024765932629122744 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 7/15 [05:28<06:15, 46.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 2.623207011800752e-05 for 13 batches\n",
            "Average Training Loss: 0.00011338518625336746 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 53%|█████▎    | 8/15 [06:15<05:28, 46.95s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 2.3330479962169193e-05 for 13 batches\n",
            "Average Training Loss: 0.0002414825088607131 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|██████    | 9/15 [07:02<04:41, 46.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 5.7026025597923974e-05 for 13 batches\n",
            "Average Training Loss: 0.00013144000771774244 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 10/15 [07:49<03:54, 46.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 2.8049866999096524e-05 for 13 batches\n",
            "Average Training Loss: 0.00011069787991857538 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|███████▎  | 11/15 [08:36<03:07, 46.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 1.7633955534480403e-05 for 13 batches\n",
            "Average Training Loss: 7.894124396017333e-05 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 12/15 [09:23<02:20, 46.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 1.5821484601894135e-05 for 13 batches\n",
            "Average Training Loss: 6.342710197628775e-05 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 13/15 [10:10<01:33, 46.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 1.3455823556376765e-05 for 13 batches\n",
            "Average Training Loss: 6.247559437184691e-05 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 93%|█████████▎| 14/15 [10:56<00:46, 46.85s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 1.6604564813440866e-05 for 13 batches\n",
            "Average Training Loss: 4.7094444208539696e-05 for 160 batches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [11:43<00:00, 46.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Validation Accuracy: 1.2552395376657772e-05 for 13 batches\n",
            "Time for training: 3530793.535888672 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_0Jwy4ZaM2S"
      },
      "source": [
        "# Evaluation and Analysis\n",
        "Now that we have our test predicitions, we "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AY_GTXYdaQY"
      },
      "source": [
        "#torch.save(model.state_dict(), \"./drive/MyDrive/Colab Notebooks/baseline_gap_model\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "837jLKVD9_GO"
      },
      "source": [
        "# Evaluating model on test data\n",
        "test_preds = evaluate_test_data(test_df, model, tokenizer, 32, torch_device)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaFYpGmrmMEs"
      },
      "source": [
        "import csv\n",
        "a_preds, b_preds, neither_preds,  = [], [], []\n",
        "a_corefs, b_corefs, corr_preds, ids = [], [], [], []\n",
        "\n",
        "for data in test_preds:\n",
        "  key = list(data.keys())[0]\n",
        "  a_pred = data[key]['prediction'][0]\n",
        "  b_pred = data[key]['prediction'][1]\n",
        "  n_pred = data[key]['prediction'][2]\n",
        "  target = data[key]['target']\n",
        "  pred_correct = False\n",
        "\n",
        "  if target == 0:\n",
        "    if a_pred > b_pred and a_pred > n_pred:\n",
        "      pred_correct = True\n",
        "      a_coref = True\n",
        "      b_coref = False\n",
        "  elif target == 1:\n",
        "    if b_pred > a_pred and b_pred > n_pred:\n",
        "      pred_correct = True\n",
        "      a_coref = False\n",
        "      b_coref = True\n",
        "  else:\n",
        "    if n_pred > a_pred and n_pred > b_pred:\n",
        "      pred_correct = True\n",
        "      a_coref = False\n",
        "      b_coref = False\n",
        "\n",
        "  a_preds.append(a_pred)\n",
        "  b_preds.append(b_pred)\n",
        "  neither_preds.append(n_pred)\n",
        "  a_corefs.append(a_coref)\n",
        "  b_corefs.append(b_coref)\n",
        "  corr_preds.append(pred_correct)\n",
        "  ids.append(key)\n",
        "\n",
        "\n",
        "\n",
        "a_preds = np.array(a_preds)\n",
        "b_preds = np.array(b_preds)\n",
        "neither_preds = np.array(neither_preds)\n",
        "\n",
        "result_df = pd.DataFrame([ids[:], a_preds[:], b_preds[:], neither_preds[:], a_corefs[:], b_corefs[:], corr_preds[:]],\n",
        "                         index=['ID', 'A', 'B', 'NEITHER', 'A-coref', 'B-coref', 'Predicted Correctly?']).transpose()\n",
        "result_df.to_csv('result.tsv', index=False, sep='\\t', quoting=csv.QUOTE_NONE)"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}